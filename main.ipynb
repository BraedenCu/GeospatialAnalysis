{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a single PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        text = \"\"\n",
    "        for page in range(reader.numPages):\n",
    "            text += reader.getPage(page).extractText()\n",
    "    return text\n",
    "\n",
    "# Function to translate text from Russian to English\n",
    "def translate_text_to_english(text, source_language='ru'):\n",
    "    if not text:  # Checks if the text is None or empty\n",
    "        return \"\"\n",
    "\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        translation = translator.translate(text, src=source_language, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # emoving extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis function\n",
    "def analyze_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        return \"Positive\", compound_score\n",
    "    elif compound_score <= -0.05:\n",
    "        return \"Negative\", compound_score\n",
    "    else:\n",
    "        return \"Neutral\", compound_score\n",
    "\n",
    "\n",
    "# Placeholder for additional analysis function\n",
    "def perform_additional_analysis(texts):\n",
    "    # Implement additional analysis techniques\n",
    "    pass\n",
    "\n",
    "def read_pdfs_from_directory(directory):\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            path = os.path.join(directory, filename)\n",
    "            russian_text = extract_text_from_pdf(path)\n",
    "            print(f\"Extracted text from {filename}: {russian_text[:100]}...\")  # Print first 100 characters\n",
    "            english_text = translate_text_to_english(russian_text)\n",
    "            texts.append(english_text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Main execution for sentiment analysis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m directory_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home//dev/dev/research/GeospatialAnalysis/library\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m texts \u001b[39m=\u001b[39m read_pdfs_from_directory(directory_path)\n\u001b[1;32m      4\u001b[0m processed_texts \u001b[39m=\u001b[39m [preprocess_text(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n\u001b[1;32m      5\u001b[0m sentiments \u001b[39m=\u001b[39m [analyze_sentiment(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m processed_texts]\n",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m, in \u001b[0;36mread_pdfs_from_directory\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     25\u001b[0m         path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, filename)\n\u001b[1;32m     26\u001b[0m         russian_text \u001b[39m=\u001b[39m extract_text_from_pdf(path)\n\u001b[0;32m---> 27\u001b[0m         english_text \u001b[39m=\u001b[39m translate_text_to_english(russian_text)\n\u001b[1;32m     28\u001b[0m         texts\u001b[39m.\u001b[39mappend(english_text)\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m texts\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m, in \u001b[0;36mtranslate_text_to_english\u001b[0;34m(text, source_language)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate_text_to_english\u001b[39m(text, source_language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mru\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     12\u001b[0m     translator \u001b[39m=\u001b[39m Translator()\n\u001b[0;32m---> 13\u001b[0m     translation \u001b[39m=\u001b[39m translator\u001b[39m.\u001b[39;49mtranslate(text, src\u001b[39m=\u001b[39;49msource_language, dest\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m translation\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/anaconda3/envs/sentiment_analysis_env/lib/python3.8/site-packages/googletrans/client.py:219\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    218\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(resp)\n\u001b[0;32m--> 219\u001b[0m parsed \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(data[\u001b[39m0\u001b[39;49m][\u001b[39m2\u001b[39;49m])\n\u001b[1;32m    220\u001b[0m \u001b[39m# not sure\u001b[39;00m\n\u001b[1;32m    221\u001b[0m should_spacing \u001b[39m=\u001b[39m parsed[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m3\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/sentiment_analysis_env/lib/python3.8/json/__init__.py:341\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(s, (\u001b[39mbytes\u001b[39m, \u001b[39mbytearray\u001b[39m)):\n\u001b[0;32m--> 341\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    342\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kw:\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
     ]
    }
   ],
   "source": [
    "# Main execution for sentiment analysis\n",
    "directory_path = \"/home//dev/dev/research/GeospatialAnalysis/library\"\n",
    "texts = read_pdfs_from_directory(directory_path)\n",
    "processed_texts = [preprocess_text(text) for text in texts]\n",
    "sentiments = [analyze_sentiment(text) for text in processed_texts]\n",
    "\n",
    "# Perform Sentiment Analysis\n",
    "for sentiment in sentiments:\n",
    "    print(f\"Sentiment: {sentiment[0]}, Compound Score: {sentiment[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spreadsheet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_and_prepare_data(filepath):\n",
    "    df = pd.read_excel(filepath)\n",
    "\n",
    "    # Assuming the file has columns like 'Year', 'Hours', and maybe others\n",
    "    # If there are other important columns or preprocessing steps, add them here\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_visualizations(df):\n",
    "    # Visualization 1: Line plot of average hours per week over the years\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df, x='Year', y='Hours')\n",
    "    plt.title('Average Hours per Week Over the Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Average Hours')\n",
    "    plt.show()\n",
    "\n",
    "    # Add more visualizations based on your data and requirements\n",
    "    # For example, if there are different categories or groups, you might\n",
    "    # want to create separate plots for each category\n",
    "\n",
    "def perform_statistical_analysis(df):\n",
    "    # Descriptive statistics\n",
    "    summary = df.describe()\n",
    "    print(\"Descriptive Statistics:\\n\", summary)\n",
    "\n",
    "    # Check for correlation\n",
    "    correlation = df.corr()\n",
    "    print(\"\\nCorrelation Matrix:\\n\", correlation)\n",
    "\n",
    "    # More complex analyses can be added here, like regression analysis, \n",
    "    # if relevant to your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filepath = '/mnt/data/Hours per week.xlsx'\n",
    "    df = read_and_prepare_data(filepath)\n",
    "    create_visualizations(df)\n",
    "    perform_statistical_analysis(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
