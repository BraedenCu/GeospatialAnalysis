{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a single PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        text = \"\"\n",
    "        for page in range(reader.numPages):\n",
    "            text += reader.getPage(page).extractText()\n",
    "    return text\n",
    "\n",
    "# Function to translate text from Russian to English\n",
    "def translate_text_to_english(text, source_language='ru'):\n",
    "    if not text:  # Checks if the text is None or empty\n",
    "        return \"\"\n",
    "\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        translation = translator.translate(text, src=source_language, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # emoving extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis function\n",
    "def analyze_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        return \"Positive\", compound_score\n",
    "    elif compound_score <= -0.05:\n",
    "        return \"Negative\", compound_score\n",
    "    else:\n",
    "        return \"Neutral\", compound_score\n",
    "\n",
    "\n",
    "# Placeholder for additional analysis function\n",
    "def perform_additional_analysis(texts):\n",
    "    # Implement additional analysis techniques\n",
    "    pass\n",
    "\n",
    "def read_pdfs_from_directory(directory):\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            path = os.path.join(directory, filename)\n",
    "            russian_text = extract_text_from_pdf(path)\n",
    "            print(f\"Extracted text from {filename}: {russian_text[:100]}...\")  # Print first 100 characters\n",
    "            english_text = translate_text_to_english(russian_text)\n",
    "            texts.append(english_text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 5th grade.pdf: МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ  \n",
      "ДОНЕЦКОЙ НАРОДНОЙ РЕСПУБЛИКИ  \n",
      " \n",
      "ГОУ ДПО «ДОНЕЦКИЙ РЕСПУБЛИКАНСКИ...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from History of Lughansk peoples republic.pdf: Луганская Народная Республика: \n",
      "история становления \n",
      "государственности\n",
      "МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НА...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from 7th grade.pdf: 1 МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ  \n",
      "ДОНЕЦКОЙ НАРОДНОЙ РЕСПУБЛИКИ  \n",
      " \n",
      "ГОУ ДПО « ДОНЕЦКИЙ РЕСПУБЛИКАН...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from History of donbas, general book.pdf: МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ  \n",
      "ДОНЕЦКОЙ НАРОДНОЙ РЕСПУБЛИКИ  \n",
      "ГОУ ВПО «Донецкий национальный уни...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from 9th grade.pdf: МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ  \n",
      "ДОНЕЦКОЙ НАРОДНОЙ РЕСПУБЛИКИ  \n",
      " \n",
      "ГОУ ДПО «ДОНЕЦКИЙ РЕСПУБЛИКАНСКИ...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from 8th garde.pdf: МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ  \n",
      "ДОНЕЦКОЙ НАРОДНОЙ РЕСПУБЛИКИ  \n",
      " \n",
      "ГОУ ДПО « ДОНЕЦКИЙ РЕСПУБЛИКАНСК...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from 11th grade part I.pdf: МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ  \n",
      "ДОНЕЦКОЙ НАРОДНОЙ РЕСПУБЛИКИ  \n",
      " \n",
      "ГОУ ДПО «ДОНЕЦКИЙ РЕСПУБЛИКАНСКИ...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from 11th grade part II.pdf: МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ  \n",
      "ДОНЕЦКОЙ НАРОДНОЙ РЕСПУБЛИКИ  \n",
      " \n",
      "ГОУ ДПО «ДОНЕЦКИЙ РЕСПУБЛИКАНСКИ...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from History of Homeland.pdf: МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ ЛНР  \n",
      "ЛУГАНСКИЙ НАЦИОНАЛЬНЫЙ УНИВЕРСИТЕТ  \n",
      "имени ВЛАДИМИРА ДАЛЯ\n",
      "Бел...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from 10th grade.pdf: МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ  \n",
      "ДОНЕЦКОЙ НАРОДНОЙ РЕСПУБЛИКИ  \n",
      " \n",
      "ГОУ ДПО «ДОНЕЦКИЙ РЕСПУБЛИКАНСКИ...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Extracted text from 6th grade.pdf: МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ  \n",
      "ДОНЕЦКОЙ НАРОДНОЙ РЕСПУБЛИКИ  \n",
      " \n",
      "ГОУ ДПО «ДОНЕЦКИЙ РЕСПУБЛИКАНСКИ...\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/home/dev/nltk_data'\n    - '/home/dev/anaconda3/envs/sentiment_analysis_env/nltk_data'\n    - '/home/dev/anaconda3/envs/sentiment_analysis_env/share/nltk_data'\n    - '/home/dev/anaconda3/envs/sentiment_analysis_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m texts \u001b[39m=\u001b[39m read_pdfs_from_directory(directory_path)\n\u001b[1;32m      4\u001b[0m processed_texts \u001b[39m=\u001b[39m [preprocess_text(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n\u001b[0;32m----> 5\u001b[0m sentiments \u001b[39m=\u001b[39m [analyze_sentiment(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m processed_texts]\n\u001b[1;32m      7\u001b[0m \u001b[39m# Perform Sentiment Analysis\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m sentiment \u001b[39min\u001b[39;00m sentiments:\n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m texts \u001b[39m=\u001b[39m read_pdfs_from_directory(directory_path)\n\u001b[1;32m      4\u001b[0m processed_texts \u001b[39m=\u001b[39m [preprocess_text(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n\u001b[0;32m----> 5\u001b[0m sentiments \u001b[39m=\u001b[39m [analyze_sentiment(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m processed_texts]\n\u001b[1;32m      7\u001b[0m \u001b[39m# Perform Sentiment Analysis\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m sentiment \u001b[39min\u001b[39;00m sentiments:\n",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m, in \u001b[0;36manalyze_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manalyze_sentiment\u001b[39m(text):\n\u001b[0;32m----> 3\u001b[0m     sia \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[1;32m      4\u001b[0m     sentiment_scores \u001b[39m=\u001b[39m sia\u001b[39m.\u001b[39mpolarity_scores(text)\n\u001b[1;32m      5\u001b[0m     compound_score \u001b[39m=\u001b[39m sentiment_scores[\u001b[39m'\u001b[39m\u001b[39mcompound\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/sentiment_analysis_env/lib/python3.8/site-packages/nltk/sentiment/vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     lexicon_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon_file \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mload(lexicon_file)\n\u001b[1;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_lex_dict()\n\u001b[1;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstants \u001b[39m=\u001b[39m VaderConstants()\n",
      "File \u001b[0;32m~/anaconda3/envs/sentiment_analysis_env/lib/python3.8/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/sentiment_analysis_env/lib/python3.8/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/anaconda3/envs/sentiment_analysis_env/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/home/dev/nltk_data'\n    - '/home/dev/anaconda3/envs/sentiment_analysis_env/nltk_data'\n    - '/home/dev/anaconda3/envs/sentiment_analysis_env/share/nltk_data'\n    - '/home/dev/anaconda3/envs/sentiment_analysis_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Main execution for sentiment analysis\n",
    "directory_path = \"/home//dev/dev/research/GeospatialAnalysis/library\"\n",
    "texts = read_pdfs_from_directory(directory_path)\n",
    "processed_texts = [preprocess_text(text) for text in texts]\n",
    "sentiments = [analyze_sentiment(text) for text in processed_texts]\n",
    "\n",
    "# Perform Sentiment Analysis\n",
    "for sentiment in sentiments:\n",
    "    print(f\"Sentiment: {sentiment[0]}, Compound Score: {sentiment[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/dev/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "from googletrans import Translator\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Function to extract text from a single PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        text = \"\"\n",
    "        for page in range(reader.numPages):\n",
    "            text += reader.getPage(page).extractText()\n",
    "    return text\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Example: Removing extra spaces\n",
    "    return text\n",
    "\n",
    "# Sentiment analysis function using NLTK's VADER\n",
    "def analyze_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        return \"Positive\", compound_score\n",
    "    elif compound_score <= -0.05:\n",
    "        return \"Negative\", compound_score\n",
    "    else:\n",
    "        return \"Neutral\", compound_score\n",
    "\n",
    "# Function to translate text from Russian to English\n",
    "def translate_text_to_english(text, source_language='ru'):\n",
    "    if not text:  # Checks if the text is None or empty\n",
    "        return \"\"\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        translation = translator.translate(text, src=source_language, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to read and process all PDFs in a directory\n",
    "def read_pdfs_from_directory(directory):\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            path = os.path.join(directory, filename)\n",
    "            text = extract_text_from_pdf(path)\n",
    "            texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    directory_path = \"/home//dev/dev/research/GeospatialAnalysis/library\"\n",
    "    texts = read_pdfs_from_directory(directory_path)\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    sentiments = [analyze_sentiment(text) for text in processed_texts]\n",
    "\n",
    "    # Translating texts after sentiment analysis\n",
    "    translated_texts = [translate_text_to_english(text) for text in processed_texts]\n",
    "\n",
    "    # Output results\n",
    "    for sentiment, translated_text in zip(sentiments, translated_texts):\n",
    "        print(f\"Sentiment: {sentiment[0]}, Compound Score: {sentiment[1]}\")\n",
    "        print(f\"Translated Text (Excerpt): {translated_text[:200]}...\\n\")  # Print first 200 characters of the translated text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spreadsheet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_and_prepare_data(filepath):\n",
    "    df = pd.read_excel(filepath)\n",
    "\n",
    "    # Assuming the file has columns like 'Year', 'Hours', and maybe others\n",
    "    # If there are other important columns or preprocessing steps, add them here\n",
    "    return df\n",
    "\n",
    "def create_visualizations(df):\n",
    "    # Visualization 1: Line plot of average hours per week over the years\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df, x='Year', y='Hours')\n",
    "    plt.title('Average Hours per Week Over the Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Average Hours')\n",
    "    plt.show()\n",
    "\n",
    "    # Add more visualizations based on your data and requirements\n",
    "    # For example, if there are different categories or groups, you might\n",
    "    # want to create separate plots for each category\n",
    "\n",
    "def perform_statistical_analysis(df):\n",
    "    # Descriptive statistics\n",
    "    summary = df.describe()\n",
    "    print(\"Descriptive Statistics:\\n\", summary)\n",
    "\n",
    "    # Check for correlation\n",
    "    correlation = df.corr()\n",
    "    print(\"\\nCorrelation Matrix:\\n\", correlation)\n",
    "\n",
    "    # More complex analyses can be added here, like regression analysis, \n",
    "    # if relevant to your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filepath = '/mnt/data/Hours per week.xlsx'\n",
    "    df = read_and_prepare_data(filepath)\n",
    "    create_visualizations(df)\n",
    "    perform_statistical_analysis(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
